import looter as lt
from pprint import pprint
from concurrent import futures

domain = ''

def crawl(url):
    src = lt.get_source(url)
    items = src.cssselect()
    for item in items:
        data = dict()
        # data['<col>'] = post.cssselect(...)
        pprint(data)
        # You can define your save_data function in advance and call it here :)


if __name__ == '__main__':
    tasklist = list()
    with futures.ThreadPoolExecutor(20) as executor:
        executor.map(crawl, tasklist)